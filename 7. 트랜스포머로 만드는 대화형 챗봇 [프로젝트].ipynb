{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "669df057",
   "metadata": {},
   "source": [
    "[루브릭]\n",
    "\n",
    "1. 한국어 전처리를 통해 학습 데이터셋을 구축하였다.\t\n",
    "    - 공백과 특수문자 처리, 토크나이징, 병렬데이터 구축의 과정이 적절히 진행되었다.\n",
    "2. 트랜스포머 모델을 구현하여 한국어 챗봇 모델 학습을 정상적으로 진행하였다.\n",
    "    - 구현한 트랜스포머 모델이 한국어 병렬 데이터 학습 시 안정적으로 수렴하였다.\n",
    "3. 한국어 입력문장에 대해 한국어로 답변하는 함수를 구현하였다.\n",
    "    - 한국어 입력문장에 맥락에 맞는 한국어로 답변을 리턴하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3de7ae",
   "metadata": {},
   "source": [
    "# 7-1. 프로젝트: 한국어 데이터로 챗봇 만들기\n",
    "\n",
    "영어로 만들었던 챗봇을 한국어 데이터로 바꿔서 훈련시켜봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65ee26cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "# 시작하기 전에 우선 주요 라이브러리 버전을 확인해 봅시다.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84936f41",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 수집하기\n",
    "\n",
    "한국어 챗봇 데이터는 송영숙님이 공개한 챗봇 데이터를 사용합니다.\n",
    "\n",
    "이 데이터는 아래의 링크에서 다운로드할 수 있습니다.\n",
    "- [songys/Chatbot_data](https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9b6a092",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p ~/aiffel/16_nlp/5_songys_chatbot/data/\n",
    "! ln -s ~/data/* ~/aiffel/16_nlp/5_songys_chatbot/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa74d940",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 전처리하기\n",
    "\n",
    "영어 데이터와는 전혀 다른 데이터인 만큼 영어 데이터에 사용했던 전처리와 일부 동일한 전처리도 필요하겠지만 전체적으로는 다른 전처리를 수행해야 할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06a94ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SD카드 망가졌어</td>\n",
       "      <td>다시 새로 사는 게 마음 편해요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SD카드 안돼</td>\n",
       "      <td>다시 새로 사는 게 마음 편해요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SNS 맞팔 왜 안하지ㅠㅠ</td>\n",
       "      <td>잘 모르고 있을 수도 있어요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SNS 시간낭비인 거 아는데 매일 하는 중</td>\n",
       "      <td>시간을 정하고 해보세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SNS 시간낭비인데 자꾸 보게됨</td>\n",
       "      <td>시간을 정하고 해보세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SNS보면 나만 빼고 다 행복해보여</td>\n",
       "      <td>자랑하는 자리니까요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>가끔 궁금해</td>\n",
       "      <td>그 사람도 그럴 거예요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>가끔 뭐하는지 궁금해</td>\n",
       "      <td>그 사람도 그럴 거예요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>가끔은 혼자인게 좋다</td>\n",
       "      <td>혼자를 즐기세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>가난한 자의 설움</td>\n",
       "      <td>돈은 다시 들어올 거예요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>가만 있어도 땀난다</td>\n",
       "      <td>땀을 식혀주세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>가상화폐 쫄딱 망함</td>\n",
       "      <td>어서 잊고 새출발 하세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>가스불 켜고 나갔어</td>\n",
       "      <td>빨리 집에 돌아가서 끄고 나오세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>가스불 켜놓고 나온거 같아</td>\n",
       "      <td>빨리 집에 돌아가서 끄고 나오세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>가스비 너무 많이 나왔다.</td>\n",
       "      <td>다음 달에는 더 절약해봐요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Q                    A  label\n",
       "0                    12시 땡!           하루가 또 가네요.      0\n",
       "1               1지망 학교 떨어졌어            위로해 드립니다.      0\n",
       "2              3박4일 놀러가고 싶다          여행은 언제나 좋죠.      0\n",
       "3           3박4일 정도 놀러가고 싶다          여행은 언제나 좋죠.      0\n",
       "4                   PPL 심하네           눈살이 찌푸려지죠.      0\n",
       "5                 SD카드 망가졌어   다시 새로 사는 게 마음 편해요.      0\n",
       "6                   SD카드 안돼   다시 새로 사는 게 마음 편해요.      0\n",
       "7            SNS 맞팔 왜 안하지ㅠㅠ     잘 모르고 있을 수도 있어요.      0\n",
       "8   SNS 시간낭비인 거 아는데 매일 하는 중        시간을 정하고 해보세요.      0\n",
       "9         SNS 시간낭비인데 자꾸 보게됨        시간을 정하고 해보세요.      0\n",
       "10      SNS보면 나만 빼고 다 행복해보여          자랑하는 자리니까요.      0\n",
       "11                   가끔 궁금해        그 사람도 그럴 거예요.      0\n",
       "12              가끔 뭐하는지 궁금해        그 사람도 그럴 거예요.      0\n",
       "13              가끔은 혼자인게 좋다            혼자를 즐기세요.      0\n",
       "14                가난한 자의 설움       돈은 다시 들어올 거예요.      0\n",
       "15               가만 있어도 땀난다            땀을 식혀주세요.      0\n",
       "16               가상화폐 쫄딱 망함       어서 잊고 새출발 하세요.      0\n",
       "17               가스불 켜고 나갔어  빨리 집에 돌아가서 끄고 나오세요.      0\n",
       "18           가스불 켜놓고 나온거 같아  빨리 집에 돌아가서 끄고 나오세요.      0\n",
       "19           가스비 너무 많이 나왔다.      다음 달에는 더 절약해봐요.      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data = pd.read_csv('data/ChatbotData .csv')\n",
    "chatbot_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7495f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11823, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5a6bf9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    11750\n",
       "True        73\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data.duplicated(subset=['Q', 'A']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afee867f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11750, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data_cleaned = chatbot_data.drop_duplicates(subset=['Q', 'A'])\n",
    "chatbot_data_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b21a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = chatbot_data_cleaned['Q'], chatbot_data_cleaned['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b19eb34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the text data\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    # Remove unwanted characters, special symbols, etc.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)  # Add spaces around punctuation\n",
    "    sentence = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9?.!,]+\", \" \", sentence)  # Only allow Hangul, numbers, and basic punctuation\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "# Apply preprocessing to both input and output columns\n",
    "questions = questions.apply(preprocess_sentence)\n",
    "answers = answers.apply(preprocess_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f283b1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         12시 땡 !\n",
       "1                     1지망 학교 떨어졌어\n",
       "2                    3박4일 놀러가고 싶다\n",
       "3                 3박4일 정도 놀러가고 싶다\n",
       "4                         PPL 심하네\n",
       "                   ...           \n",
       "11818             훔쳐보는 것도 눈치 보임 .\n",
       "11819             훔쳐보는 것도 눈치 보임 .\n",
       "11820                흑기사 해주는 짝남 .\n",
       "11821    힘든 연애 좋은 연애라는게 무슨 차이일까 ?\n",
       "11822                  힘들어서 결혼할까봐\n",
       "Name: Q, Length: 11750, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2cc1091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      하루가 또 가네요 .\n",
       "1                       위로해 드립니다 .\n",
       "2                     여행은 언제나 좋죠 .\n",
       "3                     여행은 언제나 좋죠 .\n",
       "4                      눈살이 찌푸려지죠 .\n",
       "                   ...            \n",
       "11818          티가 나니까 눈치가 보이는 거죠 !\n",
       "11819               훔쳐보는 거 티나나봐요 .\n",
       "11820                      설렜겠어요 .\n",
       "11821    잘 헤어질 수 있는 사이 여부인 거 같아요 .\n",
       "11822          도피성 결혼은 하지 않길 바라요 .\n",
       "Name: A, Length: 11750, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7de0f9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 1, 78, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_char_q = questions.apply(len).max()\n",
    "min_char_q = questions.apply(len).min()\n",
    "\n",
    "max_char_a = answers.apply(len).max()\n",
    "min_char_a = answers.apply(len).min()\n",
    "\n",
    "max_char_q, min_char_q, max_char_a, min_char_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d7ff3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9276</th>\n",
       "      <td>남자들은 좋아하는 여자가 자기보다 능력이 좋은 경우에 아무리 좋아해도 마음 접고 포...</td>\n",
       "      <td>절대 능력만 중요하지 않아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11810</th>\n",
       "      <td>확실히 좋아하는 데도 관심 있는거 티 안내려고 선톡 안하고 일부러 늦게 보내고 그러...</td>\n",
       "      <td>많이 있어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       Q                 A  \\\n",
       "9276   남자들은 좋아하는 여자가 자기보다 능력이 좋은 경우에 아무리 좋아해도 마음 접고 포...  절대 능력만 중요하지 않아요.   \n",
       "11810  확실히 좋아하는 데도 관심 있는거 티 안내려고 선톡 안하고 일부러 늦게 보내고 그러...           많이 있어요.   \n",
       "\n",
       "       label  \n",
       "9276       2  \n",
       "11810      2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data_cleaned[chatbot_data_cleaned['Q'].apply(len) >= 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "201103c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>야</td>\n",
       "      <td>네</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3620</th>\n",
       "      <td>음</td>\n",
       "      <td>음</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3621</th>\n",
       "      <td>응</td>\n",
       "      <td>네</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5099</th>\n",
       "      <td>헉</td>\n",
       "      <td>놀랐나봐요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7228</th>\n",
       "      <td>왜</td>\n",
       "      <td>궁금하네요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8783</th>\n",
       "      <td>후</td>\n",
       "      <td>힘을 내세요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8806</th>\n",
       "      <td>휴</td>\n",
       "      <td>아이구</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8824</th>\n",
       "      <td>흠</td>\n",
       "      <td>마음이 복잡한가요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Q           A  label\n",
       "2995  야           네      0\n",
       "3620  음           음      0\n",
       "3621  응           네      0\n",
       "5099  헉      놀랐나봐요.      0\n",
       "7228  왜      궁금하네요.      1\n",
       "8783  후     힘을 내세요.      1\n",
       "8806  휴         아이구      1\n",
       "8824  흠  마음이 복잡한가요.      1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data_cleaned[chatbot_data_cleaned['Q'].apply(len) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9847108c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>나는 왜 이렇게 태어났을까?</td>\n",
       "      <td>서로 다르게 태어난 이유는 저마다의 목소리를 내기 위해서예요. 자신의 목소리를 들어...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2488</th>\n",
       "      <td>세상은 불공평해</td>\n",
       "      <td>서로 다르게 태어난 이유는 저마다의 목소리를 내기 위해서예요. 자신의 목소리를 들어...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3444</th>\n",
       "      <td>왜 이렇게 태어났을까?</td>\n",
       "      <td>서로 다르게 태어난 이유는 저마다의 목소리를 내기 위해서예요. 자신의 목소리를 들어...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4104</th>\n",
       "      <td>절대불공평</td>\n",
       "      <td>서로 다르게 태어난 이유는 저마다의 목소리를 내기 위해서예요. 자신의 목소리를 들어...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5373</th>\n",
       "      <td>5년 사귄 여자친구와 이별이네. 조언부탁좀 드려</td>\n",
       "      <td>저는 저에게 투자하는 시간을 가져요. 예를들면 운동이나 여행같이 잡념을 없앨 수 있...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6598</th>\n",
       "      <td>사랑이 끝나고 나서</td>\n",
       "      <td>원망과 슬픔에 잠기는 게 아니라 그 시간들을 가만히 내려놓고 내가 어떤 사람이었는가...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6599</th>\n",
       "      <td>사랑이 끝나고 할 일?</td>\n",
       "      <td>원망과 슬픔에 잠기는 게 아니라 그 시간들을 가만히 내려놓고 내가 어떤 사람이었는가...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7339</th>\n",
       "      <td>이 명절을어찌보낼지.허허</td>\n",
       "      <td>명절에는 신경써야할 것들이 너무나 많죠. 이번 명절에는 혼자만의 여행을 떠나보는 건...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9208</th>\n",
       "      <td>나에 대한 관심이 줄어든 거 같아</td>\n",
       "      <td>연인 사이로 발전할 수 있었던 계기를 살펴보면 서로에 대한 관심이 있었기 때문이라는...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9348</th>\n",
       "      <td>남자친구가 작은 일을 다 잊어버려</td>\n",
       "      <td>작은 일은 사소한 걸로 생각해버려서 금방 잊어버리나 봅니다. 중요한 일이라는 것을 ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9413</th>\n",
       "      <td>남친 멀어진 것 같아</td>\n",
       "      <td>나를 사랑하지 않는 사람하고 계속 가는 건 나 자신을 사랑하지 않는 사람들이나 하는...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9659</th>\n",
       "      <td>랜선 연애 어떻게 생각해?</td>\n",
       "      <td>얼굴을 안 보고 부딪히지도 않아서 제대로 된 연애가 가능하지 않을 수도 있지만 다 ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9680</th>\n",
       "      <td>마음이 식은 거 같아</td>\n",
       "      <td>나를 사랑하지 않는 사람하고 계속 가는 건 나 자신을 사랑하지 않는 사람들이나 하는...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9884</th>\n",
       "      <td>사랑에 유효기간이 있을까?</td>\n",
       "      <td>사랑하는 사람과 함께하는 어제, 오늘, 내일이 각 다른 의미가 있다고 생각하면 유효...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10258</th>\n",
       "      <td>썸 탈 때 연락문제로 속상한 적 있어?</td>\n",
       "      <td>썸 탈 때는 특히 속상한 것 같아요. 내 맘 같지 않고 아직 내 사람이 아니니까요....</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10312</th>\n",
       "      <td>썸남이랑 밥 먹기로 했는데 뭐 먹자고 하지?</td>\n",
       "      <td>어떤 음식을 좋아하는지 물어보고 맛집을 찾아봐요. 그래도 이왕이면 맛있고 분위기 좋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10343</th>\n",
       "      <td>썸녀랑 밥 먹는데 어디가지?</td>\n",
       "      <td>어떤 음식을 좋아하는지 물어보고 맛집을 찾아봐요. 그래도 이왕이면 맛있고 분위기 좋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10484</th>\n",
       "      <td>어떤 연애 하고 싶어?</td>\n",
       "      <td>서로가 전부인 연애가 아니라 서로 부족한 부분을 감싸 안아서 서로를 완전하게 해주는...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10571</th>\n",
       "      <td>여자친구가 나 사랑하는 건지 확신이 없어</td>\n",
       "      <td>당신이 그녀를 사랑하면 됐지 꼭 그녀가 당신을 사랑하는지 확신이 있어야 하는 건 아...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10647</th>\n",
       "      <td>여자친구랑 연락 문제로 자주 싸워</td>\n",
       "      <td>하루종일 연락하는 걸 원하는게 아니라 바쁘더라도 중간중간에 연락해주는 걸 원하는 걸...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10728</th>\n",
       "      <td>연애는 하기 싫은데 썸은 타고 싶어.</td>\n",
       "      <td>시작하기 전 두근거림을 더 좋아하나 봐요. 아니면 연애에 대한 두려움을 가지고 있을...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10780</th>\n",
       "      <td>오늘 만나기로 했는데 빈손으로 나가?</td>\n",
       "      <td>준비하지 않아도 상관 없다고 생각해요. 필요하다면 화려하지 않고 부담 없는 선물을 ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10808</th>\n",
       "      <td>오래 못 가는 연애</td>\n",
       "      <td>연애는 문제가 있으면 문제를 해결 하기 위해 함께 노력하고 그 속에서 더욱 단단해지...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10811</th>\n",
       "      <td>오래 연애해서 그런가 관심이 줄었어</td>\n",
       "      <td>연애하다보면 서로에게 익숙해져서 처음만큼 관심을 갖지 않게 되는 게 사실일 거예요....</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10926</th>\n",
       "      <td>이상적인 연애란?</td>\n",
       "      <td>서로가 전부인 연애가 아니라 서로 부족한 부분을 감싸 안아서 서로를 완전하게 해주는...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11758</th>\n",
       "      <td>하루 종일 같이 있고 싶어</td>\n",
       "      <td>서로 떨어져서 그리워할 시간을 적당히 만들어주는 게 관계를 더 깊게 만들어 주기도 ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11759</th>\n",
       "      <td>하루 종일 붙어 있고 싶어</td>\n",
       "      <td>뭐든 함께하려는 것도 좋겠지만 오래 붙어잇는다고 해서 사랑이 더 커지거나 깊어지지 ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Q  \\\n",
       "588               나는 왜 이렇게 태어났을까?   \n",
       "2488                     세상은 불공평해   \n",
       "3444                 왜 이렇게 태어났을까?   \n",
       "4104                        절대불공평   \n",
       "5373   5년 사귄 여자친구와 이별이네. 조언부탁좀 드려   \n",
       "6598                   사랑이 끝나고 나서   \n",
       "6599                 사랑이 끝나고 할 일?   \n",
       "7339                이 명절을어찌보낼지.허허   \n",
       "9208           나에 대한 관심이 줄어든 거 같아   \n",
       "9348           남자친구가 작은 일을 다 잊어버려   \n",
       "9413                  남친 멀어진 것 같아   \n",
       "9659               랜선 연애 어떻게 생각해?   \n",
       "9680                  마음이 식은 거 같아   \n",
       "9884               사랑에 유효기간이 있을까?   \n",
       "10258       썸 탈 때 연락문제로 속상한 적 있어?   \n",
       "10312    썸남이랑 밥 먹기로 했는데 뭐 먹자고 하지?   \n",
       "10343             썸녀랑 밥 먹는데 어디가지?   \n",
       "10484                어떤 연애 하고 싶어?   \n",
       "10571      여자친구가 나 사랑하는 건지 확신이 없어   \n",
       "10647          여자친구랑 연락 문제로 자주 싸워   \n",
       "10728        연애는 하기 싫은데 썸은 타고 싶어.   \n",
       "10780        오늘 만나기로 했는데 빈손으로 나가?   \n",
       "10808                  오래 못 가는 연애   \n",
       "10811         오래 연애해서 그런가 관심이 줄었어   \n",
       "10926                   이상적인 연애란?   \n",
       "11758              하루 종일 같이 있고 싶어   \n",
       "11759              하루 종일 붙어 있고 싶어   \n",
       "\n",
       "                                                       A  label  \n",
       "588    서로 다르게 태어난 이유는 저마다의 목소리를 내기 위해서예요. 자신의 목소리를 들어...      0  \n",
       "2488   서로 다르게 태어난 이유는 저마다의 목소리를 내기 위해서예요. 자신의 목소리를 들어...      0  \n",
       "3444   서로 다르게 태어난 이유는 저마다의 목소리를 내기 위해서예요. 자신의 목소리를 들어...      0  \n",
       "4104   서로 다르게 태어난 이유는 저마다의 목소리를 내기 위해서예요. 자신의 목소리를 들어...      0  \n",
       "5373   저는 저에게 투자하는 시간을 가져요. 예를들면 운동이나 여행같이 잡념을 없앨 수 있...      1  \n",
       "6598   원망과 슬픔에 잠기는 게 아니라 그 시간들을 가만히 내려놓고 내가 어떤 사람이었는가...      1  \n",
       "6599   원망과 슬픔에 잠기는 게 아니라 그 시간들을 가만히 내려놓고 내가 어떤 사람이었는가...      1  \n",
       "7339   명절에는 신경써야할 것들이 너무나 많죠. 이번 명절에는 혼자만의 여행을 떠나보는 건...      1  \n",
       "9208   연인 사이로 발전할 수 있었던 계기를 살펴보면 서로에 대한 관심이 있었기 때문이라는...      2  \n",
       "9348   작은 일은 사소한 걸로 생각해버려서 금방 잊어버리나 봅니다. 중요한 일이라는 것을 ...      2  \n",
       "9413   나를 사랑하지 않는 사람하고 계속 가는 건 나 자신을 사랑하지 않는 사람들이나 하는...      2  \n",
       "9659   얼굴을 안 보고 부딪히지도 않아서 제대로 된 연애가 가능하지 않을 수도 있지만 다 ...      2  \n",
       "9680   나를 사랑하지 않는 사람하고 계속 가는 건 나 자신을 사랑하지 않는 사람들이나 하는...      2  \n",
       "9884   사랑하는 사람과 함께하는 어제, 오늘, 내일이 각 다른 의미가 있다고 생각하면 유효...      2  \n",
       "10258  썸 탈 때는 특히 속상한 것 같아요. 내 맘 같지 않고 아직 내 사람이 아니니까요....      2  \n",
       "10312  어떤 음식을 좋아하는지 물어보고 맛집을 찾아봐요. 그래도 이왕이면 맛있고 분위기 좋...      2  \n",
       "10343  어떤 음식을 좋아하는지 물어보고 맛집을 찾아봐요. 그래도 이왕이면 맛있고 분위기 좋...      2  \n",
       "10484  서로가 전부인 연애가 아니라 서로 부족한 부분을 감싸 안아서 서로를 완전하게 해주는...      2  \n",
       "10571  당신이 그녀를 사랑하면 됐지 꼭 그녀가 당신을 사랑하는지 확신이 있어야 하는 건 아...      2  \n",
       "10647  하루종일 연락하는 걸 원하는게 아니라 바쁘더라도 중간중간에 연락해주는 걸 원하는 걸...      2  \n",
       "10728  시작하기 전 두근거림을 더 좋아하나 봐요. 아니면 연애에 대한 두려움을 가지고 있을...      2  \n",
       "10780  준비하지 않아도 상관 없다고 생각해요. 필요하다면 화려하지 않고 부담 없는 선물을 ...      2  \n",
       "10808  연애는 문제가 있으면 문제를 해결 하기 위해 함께 노력하고 그 속에서 더욱 단단해지...      2  \n",
       "10811  연애하다보면 서로에게 익숙해져서 처음만큼 관심을 갖지 않게 되는 게 사실일 거예요....      2  \n",
       "10926  서로가 전부인 연애가 아니라 서로 부족한 부분을 감싸 안아서 서로를 완전하게 해주는...      2  \n",
       "11758  서로 떨어져서 그리워할 시간을 적당히 만들어주는 게 관계를 더 깊게 만들어 주기도 ...      2  \n",
       "11759  뭐든 함께하려는 것도 좋겠지만 오래 붙어잇는다고 해서 사랑이 더 커지거나 깊어지지 ...      2  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data_cleaned[chatbot_data_cleaned['A'].apply(len) >= 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c92e2101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>야</td>\n",
       "      <td>네</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3620</th>\n",
       "      <td>음</td>\n",
       "      <td>음</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3621</th>\n",
       "      <td>응</td>\n",
       "      <td>네</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3878</th>\n",
       "      <td>있잖아</td>\n",
       "      <td>음</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6910</th>\n",
       "      <td>야 너!</td>\n",
       "      <td>네</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Q  A  label\n",
       "2995     야  네      0\n",
       "3620     음  음      0\n",
       "3621     응  네      0\n",
       "3878   있잖아  음      0\n",
       "6910  야 너!  네      1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data_cleaned[chatbot_data_cleaned['A'].apply(len) == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a9bd11",
   "metadata": {},
   "source": [
    "## Step 3. SubwordTextEncoder 사용하기\n",
    "\n",
    "- 한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분이 알고 있습니다. \n",
    "- 하지만 여기서는 형태소 분석기가 아닌 위 실습에서 사용했던 내부 단어 토크나이저인 SubwordTextEncoder를 그대로 사용해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "736dddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine questions and answers to create the vocabulary\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions + answers, target_vocab_size=2**13)\n",
    "\n",
    "# Define start and end tokens as integers\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "# Save the size of the vocabulary\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73ebcfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [8320]\n",
      "END_TOKEN의 번호 : [8321]\n",
      "8322\n"
     ]
    }
   ],
   "source": [
    "print('START_TOKEN의 번호 :', [tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :', [tokenizer.vocab_size + 1])\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ec8609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n",
    "MAX_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1ad6110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    \n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "        # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "        # 최대 길이 이하인 경우에만 데이터셋으로 허용\n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "  \n",
    "    # 최대 길이 으로 모든 데이터셋을 패딩\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "    return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04823cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8322\n",
      "필터링 후의 질문 샘플 개수: 11750\n",
      "필터링 후의 답변 샘플 개수: 11750\n"
     ]
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f57d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d55367",
   "metadata": {},
   "source": [
    "## Step 4. 모델 구성하기\n",
    "\n",
    "위 실습 내용을 참고하여 트랜스포머 모델을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c639afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        # 각도 배열 생성\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "\n",
    "        # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        # sin과 cosine이 교차되도록 재배열\n",
    "        pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "        pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "        pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa63bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # 가중치를 정규화\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # 패딩에 마스크 추가\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # softmax적용\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21774c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name='multi_head_attention'):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth).\n",
    "        \"\"\"\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], \\\n",
    "            inputs['value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # Q, K, V에 각각 Dense를 적용합니다\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # 스케일드 닷 프로덕트 어텐션 함수\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))\n",
    "\n",
    "        # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "844acd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0d1c96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0caf771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    \n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention\")({\n",
    "            'query': inputs,\n",
    "            'key': inputs,\n",
    "            'value': inputs,\n",
    "            'mask': padding_mask\n",
    "      })\n",
    "\n",
    "    # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb86389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    # num_layers만큼 쌓아올린 인코더의 층.\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "725ac90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "    })\n",
    "\n",
    "    # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "    attention2 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "    })\n",
    "\n",
    "    # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "    # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6c2f8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "    # 패딩 마스크\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b82ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    # 인코더에서 패딩을 위한 마스크\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='enc_padding_mask')(inputs)\n",
    "\n",
    "    # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
    "    # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask,\n",
    "        output_shape=(1, None, None),\n",
    "        name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "    # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "    # 디코더에서 패딩을 위한 마스크\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='dec_padding_mask')(inputs)\n",
    "\n",
    "    # 인코더\n",
    "    enc_outputs = encoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "    # 디코더\n",
    "    dec_outputs = decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    # 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a1c8bfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    3184640     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    3712000     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8322)   2138754     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,035,394\n",
      "Trainable params: 9,035,394\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "575f4951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c9d64c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e4ed2cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7354ee35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "184/184 [==============================] - 31s 68ms/step - loss: 1.1609 - accuracy: 0.0217\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 13s 68ms/step - loss: 0.9433 - accuracy: 0.0394\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 12s 68ms/step - loss: 0.8052 - accuracy: 0.0404\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 12s 67ms/step - loss: 0.7444 - accuracy: 0.0433\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 12s 66ms/step - loss: 0.6980 - accuracy: 0.0458\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 12s 67ms/step - loss: 0.6499 - accuracy: 0.0491\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 12s 67ms/step - loss: 0.5975 - accuracy: 0.0539\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 12s 68ms/step - loss: 0.5402 - accuracy: 0.0599\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 13s 68ms/step - loss: 0.4784 - accuracy: 0.0666\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 13s 68ms/step - loss: 0.4136 - accuracy: 0.0741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7a58ede4ceb0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fb166a",
   "metadata": {},
   "source": [
    "## Step 5. 모델 평가하기\n",
    "\n",
    "Step 1에서 선택한 전처리 방법을 고려하여 입력된 문장에 대해서 대답을 얻는 예측 함수를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fccb13cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "    # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "    sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "    # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "    # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "    # 디코더의 인퍼런스 단계\n",
    "    for i in range(MAX_LENGTH):\n",
    "        # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "        predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "\n",
    "        # 현재 예측한 단어의 정수\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "        # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "        # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output_sequence, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0b6b0f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence):\n",
    "    # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "    prediction = decoder_inference(sentence)\n",
    "\n",
    "    # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    print('입력 : {}'.format(sentence))\n",
    "    print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cbef1755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 안녕하세요?\n",
      "출력 : 저는 위로봇입니다 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'저는 위로봇입니다 .'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('안녕하세요?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "48eb19ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 오늘 저녁에는 뭘 먹을까요?\n",
      "출력 : 맛있는 거 드세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'맛있는 거 드세요 .'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the chatbot\n",
    "sentence_generation(\"오늘 저녁에는 뭘 먹을까요?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823c38f3",
   "metadata": {},
   "source": [
    "# Comment\n",
    "\n",
    "* 트랜스포머의 구조에 대해서 이해하는 것에 많은 시간을 할애했습니다. \n",
    "* 처음부터 이해가 잘 가지는 않았지만 기술이 발전해나가는 방향성을 알게 되어서 기뻤어요. \n",
    "* 처음에는 잘 안 됐지만 다양한 미디어를 통해 프로세스를 여러 번 복습하고 다시 돌아와서 보니 하나씩 퍼즐처럼 맞춰지는 것이 신기했습니다 :) \n",
    "* 복잡한 문장은 어렵겠지만, 이렇게 간단하게 챗봇을 만들어본 게 재밌었습니다! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
